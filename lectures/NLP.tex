
% Lecture 8
\subsection{NLP}
\label{sub:nlp}
    \textbf{Word embeddings}: Map symbols over vocabulary $\cV$ to vector representation $\xrightarrow{}$ embedding into a (Euclidean) vector space. Symbolic $\cV\ni w \overset{\text{embed}}{\mapsto}\vx_w\in\R^d$ quantitative. ($w_i\mapsto\vx_i\in\R^D$)
    
    \textbf{Bilinear model:} pairwise word co-occurence (pointwise mutual info):
    
    $\text{pmi}(v,w)=\log\frac{p(v,w}{p(v)p(w)}=\log\frac{p(v|w)}{p(v)}=\vx_v^\top\vx_w + \text{const}$
    
    \textbf{Skip-gram}: Pairwise occurrences of words in context window of size $R$. Text is a long sequence of words, $\vw=(w_1,\ldots,w_T)$. Co-occurrence index set: $\cC_R\coloneqq\{(i,j)\in \lbrack 1:T\rbrack^2:1\leq\lvert i-j\rvert\leq R\}$. Skip-gram objective function: $\cL(\theta;\vw)=\sum\limits_t\sum\limits_{l=-c\neq 0}^c\log\lbrack{p_\theta (w_{t+l}|w_t)}\rbrack$. Model (soft-max): 
    
    \tab$\log p_\theta(v|w)=\vx_v\T\vz_w-\log\underbrace{\sum_{u\in\cV}\exp\lbrack\vx_u\T\vz_w\rbrack}_\text{partition function}$
    
    In practice it's impractical to calculate the partition function. Use negative sampling and re-formulate relative word frequency $p(w)$ as logistic regression instead, $p_n(w)=p(w)^\alpha$:
    
    \tab$\cL(\theta;\vw)=\sum\limits_{(i,j)\in\cC_\R}\bigr\lbrack\underbrace{\log\sigma(\vx_{w_i}\T\vz_{w_j})}_\text{positive examples}+k\vE_{v\sim p_n}\underbrace{\lbrack\log\sigma(-\vx_{w_i}\T\vz_v)\rbrack}_\text{negative examples}\bigr\rbrack$
    
    \textbf{From words to sequences of words}: language models via embeddings
    
    \tab$\log p(v|\vw\coloneqq w_1,\ldots,w_{t-1})=\vx_v\T\vz_\vw+\text{ const}$, with $\vx_v$ word embedding, $\vz_\vw$ sequence embedding.
    
    \textbf{Three approaches:} \textbf{1)} \textbf{CNN}: concepually simple, fast to train but limited range memory. \textbf{2)} \textbf{Recurrent networks}: active memory management via gated units but more difficult to optimize and larger datasets needed. \textbf{3)} \textbf{Recursive networks}: in combination with parsers.
    
    %useful link for skip-gram: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/
    \subsubsection{Conv. Nets for Natural Language}
    \label{ssub:convnetsfornl}
    Map word sequence to vector sequence, get sentence matrix. Concatenate vectors. Convolve (nonlinearity is usually tanh or ReLU). Pooling (reduce each channel of variable length to single number). Predict words via softmax.
    $p(v|\vw)=\frac{\exp(\langle\vy_v,\vz_\vw\rangle)}{\sum_{u\in\cV}\exp(\langle\vy_u,\vz_\vw\rangle)}$
    uses word embeddings $\vy_v\in\R^k$.
    \subsubsection{Recurrent Networks}
    \label{ssub:recurrentnets}
    
    \textbf{Linear dynamical system}: $\bar{F}(\vh,\vx;\theta)\coloneqq\vW \vh+\vU \vx + \vb,\>\theta=(\vU,\vW,\vb,\ldots)$
    
    As always, compose with element-wise non-linearity:\\ \tab$F\coloneqq\sigma\circ\bar{F},\>\sigma\in\{\text{logistic, tanh, ReLU,\ldots}\}$
    
    Optionally produce outputs: $\vy=H(\vh;\theta)\coloneqq\sigma(\vV\vh+\vc),\>\theta=(\ldots,\vV,\vc)$
    
    Loss functions, 2 settings: \textbf{output at end} $(t=T)$: $\vh^T\mapsto H(\vh^T;\theta)=\vy$
    
   \textbf{ output at every time step}: seq $(\vy^1,\ldots,\vy^T)$:\\ \tab$\cR(\vy^1,\ldots,\vy^T)=\sum\limits^T_{t=1}\cR(\vy^t)=\sum\limits^T_{t=1}\cR(H(\vh^t;\theta))$
   (additive loss function)
    
    \textbf{Backprop through time (BPTT)}: Unroll network, define shortcut\\ \tab$\dot{\sigma}\coloneqq\sigma'(\bar{F}_i(h^{t-1},x^t))$, then\\
    
    \tab$\frac{\partial\cR}{\partial w_{ij}}=\sum\limits_{t=1}^{T}\frac{\partial \cR}{\partial h^t_i}\cdot\frac{\partial h^{t}_{i}}{\partial w_{ij}}=\sum\limits_{t=1}^{T}\frac{\partial\cR}{\partial h^t_i}\cdot\dot{\sigma}^t_i\cdot h^{t-1}_{j}$\\
    \tab$\frac{\partial\cR}{\partial u_{ik}}=\sum\limits_{t=1}^{T}\frac{\partial \cR}{\partial h^t_i}\cdot\frac{\partial h^{t}_{i}}{\partial u_{ik}}=\sum\limits_{t=1}^{T}\frac{\partial\cR}{\partial h^t_i}\cdot\dot{\sigma}^t_i\cdot x^{t}_{k}$ (notice weight sharing)
    
    \textbf{Exploding/Vanishing gradients}: spectral norm of a matrix = largest singular value $\sigma_{\max}(\vA)$. If $\sigma_{\max}(\vW)<1$, gradients are vanishing, if $\sigma_{\max}(\vJ_F) >1$, gradients may explode.
